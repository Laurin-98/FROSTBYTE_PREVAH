{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "95edec3c1e6c1978"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert CAMELS data to a readable form for Frostbyte",
   "id": "de4131fb489e83e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:50:51.884613Z",
     "start_time": "2024-11-04T12:50:51.870614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ],
   "id": "b75ac0b455290f8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:12.073226Z",
     "start_time": "2024-11-04T12:50:51.917077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CAMELES Data\n",
    "CAMELS_data_path = Path('../CH_data/CAMELS')\n",
    "# Automatisch alle CSV-Dateien im Ordner auflisten\n",
    "csv_files = list(CAMELS_data_path.glob('*.csv'))\n",
    "\n",
    "# Leere Listen für Daten, Station IDs und Zeit\n",
    "data_list = []\n",
    "station_ids = []\n",
    "time_list = []\n",
    "\n",
    "# Alle CSV-Dateien durchgehen und Daten sammeln\n",
    "for file_path in csv_files:\n",
    "    # Extrahiere die Station-ID aus dem Dateinamen\n",
    "    station_id = file_path.stem.split('_')[-1]\n",
    "    station_ids.append(station_id)\n",
    "\n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #print(f'Processing {file_path.name} with columns: {df.columns}')\n",
    "\n",
    "    # Annahme: Die CSV hat eine Spalte 'date', die umgewandelt wird\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Füge Zeitwerte zur Zeitliste hinzu, falls sie nicht leer ist\n",
    "    if not time_list:  # Überprüfen, ob time_list leer ist\n",
    "        time_list = df['date'].tolist()  # Konvertiere zu einer Liste\n",
    "\n",
    "    # Sammle die Daten für diese Station\n",
    "    data_list.append(df[['discharge_vol(m3/s)', 'waterlevel(m)', 'precipitation(mm/d)',\n",
    "                         'temperature_min(°C)', 'temperature_mean(°C)', 'temperature_max(°C)',\n",
    "                         'rel_sun_dur(%)', 'swe(mm)']].values)\n",
    "\n",
    "# Staple die Daten entlang der Stationsachse (axis=1)\n",
    "data_array = np.stack(data_list, axis=1)\n",
    "# Automatisch alle CSV-Dateien im Ordner auflisten\n",
    "csv_files = list(CAMELS_data_path.glob('*.csv'))\n",
    "\n",
    "# Leere Listen für Daten, Station IDs und Zeit\n",
    "data_list = []\n",
    "station_ids = []\n",
    "time_list = []\n",
    "\n",
    "# Alle CSV-Dateien durchgehen und Daten sammeln\n",
    "for file_path in csv_files:\n",
    "    # Extrahiere die Station-ID aus dem Dateinamen\n",
    "    station_id = file_path.stem.split('_')[-1]\n",
    "    station_ids.append(station_id)\n",
    "\n",
    "    # Lade die CSV-Datei\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    #print(f'Processing {file_path.name} with columns: {df.columns}')\n",
    "\n",
    "    # Annahme: Die CSV hat eine Spalte 'date', die umgewandelt wird\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Füge Zeitwerte zur Zeitliste hinzu, falls sie nicht leer ist\n",
    "    if not time_list:  # Überprüfen, ob time_list leer ist\n",
    "        time_list = df['date'].tolist()  # Konvertiere zu einer Liste\n",
    "\n",
    "    # Sammle die Daten für diese Station\n",
    "    data_list.append(df[['discharge_vol(m3/s)', 'waterlevel(m)', 'precipitation(mm/d)',\n",
    "                         'temperature_min(°C)', 'temperature_mean(°C)', 'temperature_max(°C)',\n",
    "                         'rel_sun_dur(%)', 'swe(mm)']].values)\n",
    "\n",
    "# Staple die Daten entlang der Stationsachse (axis=1)\n",
    "data_array = np.stack(data_list, axis=1)\n",
    "\n",
    "# Dataset erstellen\n",
    "Camels_dataset = xr.Dataset(\n",
    "    {\n",
    "        'discharge_vol': (('time', 'Station_ID'), data_array[:, :, 0]),\n",
    "        'waterlevel': (('time', 'Station_ID'), data_array[:, :, 1]),\n",
    "        'precipitation': (('time', 'Station_ID'), data_array[:, :, 2]),\n",
    "        'temperature_min': (('time', 'Station_ID'), data_array[:, :, 3]),\n",
    "        'temperature_mean': (('time', 'Station_ID'), data_array[:, :, 4]),\n",
    "        'temperature_max': (('time', 'Station_ID'), data_array[:, :, 5]),\n",
    "        'rel_sun_dur': (('time', 'Station_ID'), data_array[:, :, 6]),\n",
    "        'swe': (('time', 'Station_ID'), data_array[:, :, 7])\n",
    "    },\n",
    "    coords={\n",
    "        'time': ('time', time_list),  # Zeit ist jetzt korrekt definiert\n",
    "        'Station_ID': ('Station_ID', station_ids)  # Korrektur hier: fügen Sie die Dimension an\n",
    "    }\n",
    ")\n",
    "\n",
    "# Längen- und Breitengrad Koordinaten aus CSV hinzufügen\n",
    "coordinates_df = pd.read_csv('../CH_data/gauge_coordinates.csv', dtype={'gauge_id': str})\n",
    "\n",
    "\n",
    "# Konvertiere station_ids in string\n",
    "station_ids = [str(station_id) for station_id in station_ids]  # Konvertiere in str\n",
    "\n",
    "\n",
    "# Erstelle die Koordinaten als DataArrays, bevor du die Werte hinzufügst\n",
    "lon_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lon')\n",
    "lat_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lat')\n",
    "\n",
    "# Füge die Koordinaten zum Dataset hinzu\n",
    "Camels_dataset = Camels_dataset.assign_coords(lon=lon_array, lat=lat_array)\n",
    "\n",
    "# Füge Koordinaten in das Dataset ein\n",
    "for index, row in coordinates_df.iterrows():\n",
    "    gauge_id = str(row['gauge_id'])  # Konvertiere gauge_id in String\n",
    "    if gauge_id in station_ids:  # Überprüfen der IDs\n",
    "        # Fügen Sie Koordinaten zu den vorhandenen Koordinaten hinzu\n",
    "        Camels_dataset['lon'].loc[gauge_id] = row['lon']\n",
    "        Camels_dataset['lat'].loc[gauge_id] = row['lat']\n",
    "    else:\n",
    "        print(f\"Gauge ID {gauge_id} not found in station_ids.\")\n",
    "print('this is Camels_dataset:')\n",
    "print(Camels_dataset)"
   ],
   "id": "3aeb47ff09195af3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is Camels_dataset:\n",
      "<xarray.Dataset>\n",
      "Dimensions:           (Station_ID: 331, time: 14610)\n",
      "Coordinates:\n",
      "  * time              (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "  * Station_ID        (Station_ID) <U4 '2004' '2007' '2009' ... '6010' '6011'\n",
      "    lon               (Station_ID) float64 7.117 6.324 6.889 ... 8.579 8.649\n",
      "    lat               (Station_ID) float64 46.93 46.67 46.35 ... 45.94 45.7\n",
      "Data variables:\n",
      "    discharge_vol     (time, Station_ID) float64 nan nan 49.8 ... nan nan nan\n",
      "    waterlevel        (time, Station_ID) float64 429.0 nan 374.4 ... nan nan nan\n",
      "    precipitation     (time, Station_ID) float64 0.89 3.17 1.83 ... 0.15 0.46\n",
      "    temperature_min   (time, Station_ID) float64 -1.63 -6.33 ... nan -10.32\n",
      "    temperature_mean  (time, Station_ID) float64 2.02 -2.42 -6.89 ... nan -6.76\n",
      "    temperature_max   (time, Station_ID) float64 3.68 0.46 -2.84 ... nan -3.65\n",
      "    rel_sun_dur       (time, Station_ID) float64 0.84 0.05 31.12 ... nan 63.69\n",
      "    swe               (time, Station_ID) float64 nan nan nan ... 80.0 102.8\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:12.369986Z",
     "start_time": "2024-11-04T12:51:12.198408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Qobs Cameles\n",
    "Qobs_Camels = Camels_dataset.copy()\n",
    "\n",
    "# Behalte nur die Variable 'discharge_vol' und benenne sie in 'Flow' um\n",
    "Qobs_Camels = Qobs_Camels[['discharge_vol']].rename({'discharge_vol': 'Flow'})\n",
    "\n",
    "n_stations = Qobs_Camels.dims['Station_ID']\n",
    "\n",
    "# Erstelle ein Array mit der Länge von 'Station_ID', gefüllt mit dem Wert 'CAMELS'\n",
    "source = np.full(n_stations, 'CAMELS')\n",
    "# source als Koordinate hinzufügen\n",
    "Qobs_Camels = Qobs_Camels.assign_coords(source=('Station_ID', source))\n",
    "\n",
    "# Ausgabe des neuen Datasets\n",
    "print('this is Qobs_Camels:')\n",
    "print(Qobs_Camels)\n",
    "\n",
    "\n",
    "# Extract flow data for station with Station_ID '2319'\n",
    "try:\n",
    "    flow_2319 = Qobs_Camels.sel(Station_ID='2319')['Flow']  # Verwende '2319' als String\n",
    "    flow_2319_array = flow_2319.values\n",
    "    print(flow_2319_array)\n",
    "except KeyError:\n",
    "    print(\"Station_ID '2319' not found in Qobs_Camels.\")\n",
    "\n",
    "##\n",
    "Qobs_Camels.to_netcdf('../CH_data/CH_input_data/Qobs_Camels.nc')\n"
   ],
   "id": "ad4030c250cbe348",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is Qobs_Camels:\n",
      "<xarray.Dataset>\n",
      "Dimensions:     (Station_ID: 331, time: 14610)\n",
      "Coordinates:\n",
      "    lat         (Station_ID) float64 46.93 46.67 46.35 ... 45.96 45.94 45.7\n",
      "  * Station_ID  (Station_ID) <U4 '2004' '2007' '2009' ... '6009' '6010' '6011'\n",
      "  * time        (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "    lon         (Station_ID) float64 7.117 6.324 6.889 ... 8.525 8.579 8.649\n",
      "    source      (Station_ID) <U6 'CAMELS' 'CAMELS' ... 'CAMELS' 'CAMELS'\n",
      "Data variables:\n",
      "    Flow        (time, Station_ID) float64 nan nan 49.8 26.26 ... nan nan nan\n",
      "[0.196 0.195 0.195 ... 0.243 0.24  0.238]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:13.264108Z",
     "start_time": "2024-11-04T12:51:12.527100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# SWE Camels\n",
    "SWE_Camels = Camels_dataset.copy()\n",
    "#P_Camels = Camels_dataset.copy()\n",
    "\n",
    "SWE_Camels = SWE_Camels[['swe']].rename({'swe': 'snw'})\n",
    "#P_Camels = P_Camels[['precipitation']].rename({'precipitation': 'p'})\n",
    "\n",
    "SWE_Camels = SWE_Camels.drop_vars(['lat', 'lon'])\n",
    "#\n",
    "\n",
    "# Längen- und Breitengrad Koordinaten aus CSV hinzufügen\n",
    "coordinates_df = pd.read_csv('../CH_data/centroids_coordinates.csv', dtype={'gauge_id': str})\n",
    "\n",
    "# Konvertiere station_ids in string\n",
    "station_ids = [str(station_id) for station_id in station_ids]  # Konvertiere in str\n",
    "\n",
    "# test delete me later\n",
    "# Erstelle die Koordinaten als DataArrays, bevor du die Werte hinzufügst\n",
    "lon_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lon')\n",
    "lat_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lat')\n",
    "\n",
    "# Füge die Koordinaten zum Dataset hinzu\n",
    "SWE_Camels = SWE_Camels.assign_coords(lon=lon_array, lat=lat_array)\n",
    "#\n",
    "\n",
    "# Füge Koordinaten in das Dataset ein\n",
    "for index, row in coordinates_df.iterrows():\n",
    "    gauge_id = str(row['gauge_id'])  # Konvertiere gauge_id in String\n",
    "    if gauge_id in station_ids:  # Überprüfen der IDs\n",
    "        # Fügen Sie Koordinaten zu den vorhandenen Koordinaten hinzu\n",
    "        SWE_Camels['lon'].loc[gauge_id] = row['lon']\n",
    "        SWE_Camels['lat'].loc[gauge_id] = row['lat']\n",
    "        #P_Camels['lon'].loc[gauge_id] = row['lon']\n",
    "        #P_Camels['lat'].loc[gauge_id] = row['lat']\n",
    "    else:\n",
    "        print(f\"Gauge ID {gauge_id} not found in station_ids. (SWE)\")\n",
    "\n",
    "\n",
    "# Füge die station_name Koordinate hinzu\n",
    "station_names = [f\"{station_id}_centroid\" for station_id in SWE_Camels.coords['Station_ID'].values]\n",
    "#\n",
    "\n",
    "# Erstelle die station_name Koordinate als DataArray\n",
    "station_name_array = xr.DataArray(station_names, coords=[SWE_Camels.coords['Station_ID']], dims='Station_ID', name='station_name')\n",
    "\n",
    "\n",
    "# Füge die station_name Koordinate zum Dataset hinzu\n",
    "SWE_Camels = SWE_Camels.assign_coords(station_name=station_name_array)\n",
    "\n",
    "\n",
    "# Ausgabe des neuen Datasets mit der station_name Koordinate\n",
    "\n",
    "SWE_Camels = SWE_Camels.rename({'Station_ID': 'station_id'})\n",
    "SWE_Camels = SWE_Camels.rename({'snw': 'swe'})\n",
    "\n",
    "# Transformer-Objekt für die Umrechnung von WGS84 (EPSG:4326) nach LV95 (EPSG:2056)\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:21781\", always_xy=True)\n",
    "\n",
    "# Angenommen, dein Dataset heißt \"SWE_Camels\"\n",
    "# Wandle lon und lat von WGS84 zu LV95 um\n",
    "lon_lv95, lat_lv95 = transformer.transform(SWE_Camels['lon'].values, SWE_Camels['lat'].values)\n",
    "\n",
    "# Überschreibe die ursprünglichen lon und lat Koordinaten im Dataset\n",
    "SWE_Camels = SWE_Camels.assign_coords(lon=(\"station_id\", lon_lv95), lat=(\"station_id\", lat_lv95))\n",
    "\n",
    "\n",
    "print(SWE_Camels)\n"
   ],
   "id": "7fe81938ab72af0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:       (station_id: 331, time: 14610)\n",
      "Coordinates:\n",
      "  * station_id    (station_id) <U4 '2004' '2007' '2009' ... '6009' '6010' '6011'\n",
      "  * time          (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "    lon           (station_id) float64 5.587e+05 5.049e+05 ... 6.934e+05\n",
      "    lat           (station_id) float64 1.753e+05 1.593e+05 ... 1.138e+05\n",
      "    station_name  (station_id) <U13 '2004_centroid' ... '6011_centroid'\n",
      "Data variables:\n",
      "    swe           (time, station_id) float64 nan nan nan ... 114.0 80.0 102.8\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:13.423330Z",
     "start_time": "2024-11-04T12:51:13.347532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Angenommen, dein ursprüngliches Dataset heißt `SWE_Camels`\n",
    "# Kopiere das Dataset, um Änderungen vorzunehmen\n",
    "new_ds = SWE_Camels.copy()\n",
    "\n",
    "# 1. `station_id` in `Station_ID` umbenennen und als Zeichenkette verwenden\n",
    "new_ds = new_ds.rename({'station_id': 'Station_ID'})\n",
    "new_ds = new_ds.assign_coords(Station_ID=(\"Station_ID\", new_ds['Station_ID'].values.astype(str)))\n",
    "\n",
    "# 2. Projektionstyp hinzufügen, falls dieser fehlt\n",
    "# Beispielwert für `Projection_Type` als Skalar hinzufügen\n",
    "new_ds['Projection_Type'] = xr.DataArray(data=0.0)  # Skalar ohne Dimensionsangabe\n",
    "\n",
    "# Überprüfen, dass `lon` und `lat` in der richtigen Form sind\n",
    "new_ds['lon'] = new_ds['lon']\n",
    "new_ds['lat'] = new_ds['lat']\n",
    "\n",
    "# Zeige das modifizierte Dataset an\n",
    "new_ds = new_ds.drop_vars('station_name')\n",
    "new_ds = new_ds.rename_dims({'Station_ID': 'station_id'})\n",
    "print(new_ds)\n",
    "new_ds.to_netcdf('../CH_data/CH_input_data/SWE_Camels.nc')\n"
   ],
   "id": "cf89701830dc9e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:          (station_id: 331, time: 14610)\n",
      "Coordinates:\n",
      "    Station_ID       (station_id) <U4 '2004' '2007' '2009' ... '6010' '6011'\n",
      "  * time             (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "    lon              (station_id) float64 5.587e+05 5.049e+05 ... 6.934e+05\n",
      "    lat              (station_id) float64 1.753e+05 1.593e+05 ... 1.138e+05\n",
      "Dimensions without coordinates: station_id\n",
      "Data variables:\n",
      "    swe              (time, station_id) float64 nan nan nan ... 114.0 80.0 102.8\n",
      "    Projection_Type  float64 0.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:14.793472Z",
     "start_time": "2024-11-04T12:51:14.275077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#P_Camels\n",
    "P_Camels = Camels_dataset.copy()\n",
    "P_Camels = P_Camels[['precipitation']]#.rename({'precipitation': 'p'})\n",
    "P_Camels = P_Camels.drop_vars(['lat', 'lon'])\n",
    "# Längen- und Breitengrad Koordinaten aus CSV hinzufügen\n",
    "coordinates_df = pd.read_csv('../CH_data/centroids_coordinates.csv', dtype={'gauge_id': str})\n",
    "\n",
    "# Konvertiere station_ids in string\n",
    "station_ids = [str(station_id) for station_id in station_ids]  # Konvertiere in str\n",
    "\n",
    "# test delete me later\n",
    "# Erstelle die Koordinaten als DataArrays, bevor du die Werte hinzufügst\n",
    "lon_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lon')\n",
    "lat_array = xr.DataArray(np.nan, coords=[station_ids], dims='Station_ID', name='lat')\n",
    "\n",
    "# Füge die Koordinaten zum Dataset hinzu\n",
    "P_Camels = P_Camels.assign_coords(lon=lon_array, lat=lat_array)\n",
    "\n",
    "# Füge Koordinaten in das Dataset ein\n",
    "for index, row in coordinates_df.iterrows():\n",
    "    gauge_id = str(row['gauge_id'])  # Konvertiere gauge_id in String\n",
    "    if gauge_id in station_ids:  # Überprüfen der IDs\n",
    "        # Fügen Sie Koordinaten zu den vorhandenen Koordinaten hinzu\n",
    "        P_Camels['lon'].loc[gauge_id] = row['lon']\n",
    "        P_Camels['lat'].loc[gauge_id] = row['lat']\n",
    "    else:\n",
    "        print(f\"Gauge ID {gauge_id} not found in station_ids. (\"\n",
    "              f\"p)\")\n",
    "station_names = [f\"{station_id}_centroid\" for station_id in P_Camels.coords['Station_ID'].values]\n",
    "station_name_array = xr.DataArray(station_names, coords=[P_Camels.coords['Station_ID']], dims='Station_ID', name='station_name')\n",
    "P_Camels = P_Camels.assign_coords(station_name=station_name_array)\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:2056\", always_xy=True)\n",
    "\n",
    "# Wandle lon und lat von WGS84 zu LV95 um\n",
    "lon_lv95, lat_lv95 = transformer.transform(P_Camels['lon'].values, P_Camels['lat'].values)\n",
    "\n",
    "# Überschreibe die ursprünglichen lon und lat Koordinaten im Dataset\n",
    "P_Camels = P_Camels.assign_coords(lon=(\"station_id\", lon_lv95), lat=(\"station_id\", lat_lv95))\n",
    "\n",
    "# 1. Definiere die Werte für \"lle\" als Objekt (Koordinatenbezeichner)\n",
    "lle_coords = [\"lon\", \"lat\", \"elev\"]\n",
    "\n",
    "# 2. Initialisiere \"elev\" mit Nullen\n",
    "elev_values = np.zeros(P_Camels.dims['Station_ID'])  # 'elev' mit Nullen auffüllen\n",
    "\n",
    "# 3. Staple die Koordinaten in einem Array und erstelle die \"LLE\"-Variable\n",
    "lle_data = np.stack([P_Camels['lon'].values, P_Camels['lat'].values, elev_values], axis=0)\n",
    "\n",
    "# 4. Erstelle das neue Dataset, füge die lle-Koordinate als Objekt und die LLE-Datenvariable hinzu\n",
    "transformed_ds = xr.Dataset(\n",
    "    {\n",
    "        \"LLE\": ([\"station\", \"lle\"], lle_data.T),  # .T für Transponieren, um die Dimensionen richtig zu setzen\n",
    "        \"precipitation\": ([\"nday\", \"station\"], P_Camels['precipitation'].values)\n",
    "    },\n",
    "    coords={\n",
    "        \"station\": P_Camels[\"Station_ID\"].values,  # station als eindimensionale Koordinate\n",
    "        \"nday\": P_Camels[\"time\"].values,           # nday als eindimensionale Koordinate\n",
    "        \"lle\": lle_coords                          # lle als Objektkoordinate\n",
    "    }\n",
    ")\n",
    "\n",
    "P_Camels = transformed_ds\n",
    "print(P_Camels)\n",
    "P_Camels.to_netcdf('../CH_data/CH_input_data/P_Camels.nc')"
   ],
   "id": "7dce7ea57f960e3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:        (lle: 3, nday: 14610, station: 331)\n",
      "Coordinates:\n",
      "  * station        (station) <U4 '2004' '2007' '2009' ... '6009' '6010' '6011'\n",
      "  * nday           (nday) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "  * lle            (lle) <U4 'lon' 'lat' 'elev'\n",
      "Data variables:\n",
      "    LLE            (station, lle) float64 2.559e+06 1.175e+06 ... 1.114e+06 0.0\n",
      "    precipitation  (nday, station) float64 0.89 3.17 1.83 ... 0.11 0.15 0.46\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:51:14.889774Z",
     "start_time": "2024-11-04T12:51:14.875816Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1fbfe444b756bd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
